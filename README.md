ğŸ“š RAG Pipeline â€“ Retrieval Augmented Generation

A complete Retrieval-Augmented Generation (RAG) pipeline built using Python, LangChain / LangGraph, Groq LLM, and a Vector Database to enable accurate, context-aware question answering over custom documents.

ğŸš€ Features

ğŸ“„ Load data from PDF / CSV / Text

âœ‚ï¸ Intelligent text chunking

ğŸ§  Embeddings generation

ğŸ—‚ï¸ Persistent Vector Database (Chroma / FAISS)

ğŸ” Semantic similarity search

ğŸ¤– LLM response generation using Groq (LLaMA)

ğŸ§µ Optional LangGraph memory for conversational RAG

ğŸ” Secure API key handling using environment variables

ğŸ§  What is RAG?

Retrieval-Augmented Generation (RAG) improves LLM responses by:

Retrieving relevant documents from a vector database

Injecting them as context into the LLM prompt

Generating accurate, grounded answers

ğŸ“Œ This avoids hallucinations and improves reliability.

ğŸ—ï¸ Project Architecture

User Query
   â†“
Embedding Model
   â†“
Vector Database (Chroma / FAISS)
   â†“
Relevant Chunks
   â†“
Prompt + Context
   â†“
Groq LLM
   â†“
Final Answer


ğŸ§° Tech Stack
Component	Technology
Language	Python
LLM	Groq (LLaMA-3 / LLaMA-3.3)
Framework	LangChain / LangGraph
Vector DB	Chroma / FAISS
Embeddings	Sentence Transformers
Data	PDF, CSV, Text
Environment	Virtualenv
